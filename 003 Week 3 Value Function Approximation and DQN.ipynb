{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectures\n",
    " - [Value functions approximation - RL by David Silver](https://www.youtube.com/watch?v=UoPei5o4fps&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ&index=6)\n",
    "    - Differentiable function approximators\n",
    "    - Incremental methods\n",
    "    - Batch methods (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading:\n",
    " - [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/pdf/1312.5602.pdf)\n",
    " - [Human-level control through deep reinforcement\n",
    "learning](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)\n",
    "  - Preprocessing.\n",
    "  - Deep Q-learning.\n",
    "  - Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding\n",
    "This week we will apply Deep Q-Networks (DQN) to Pong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the environment:\n",
    "- https://gym.openai.com/envs/Pong-v0/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### install env: \n",
    " - pip install atari-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imresize\n",
    "import random \n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor():\n",
    "    def __init__(self, game, h, w, device, n_frame=4):\n",
    "        self.game = game\n",
    "        self.h, self.w = h, w\n",
    "        self.set_box(game)\n",
    "        self.n_frame = n_frame\n",
    "        self.history = []\n",
    "        self.device = device\n",
    "        self.delta_value = 256\n",
    "    def set_box(self, game):\n",
    "        if self.game == 'pong':\n",
    "            self.box = [35, 193, 0, self.w]\n",
    "        \n",
    "    def phi(self, rbg, to_tensor=True):\n",
    "        stack = None\n",
    "        frame = self.rgb2gray(rbg)\n",
    "        self.history.append(frame)\n",
    "        if len(self.history) == self.n_frame:\n",
    "            stack = np.stack(self.history)\n",
    "            if to_tensor:\n",
    "                stack = self.to_tensor([stack])\n",
    "        self.history = self.history[-(self.n_frame-1):]\n",
    "        return stack\n",
    "    \n",
    "    def to_tensor(self, frames):\n",
    "        frame = torch.from_numpy(np.stack(frames)).float().to(self.device)\n",
    "        return frame\n",
    "    \n",
    "    def rgb2gray(self, rgb):\n",
    "        rgb = rgb[self.box[0]:self.box[1], self.box[2]: self.box[3]]\n",
    "        rgb = np.array(Image.fromarray(rgb).resize((84,84)))\n",
    "        gray = np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n",
    "        gray = gray / self.delta_value\n",
    "        return gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor('pong', 210, 160, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pong-v0\")\n",
    "observation = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 NOOP\n",
      "1 FIRE\n",
      "2 RIGHT\n",
      "3 LEFT\n",
      "4 RIGHTFIRE\n",
      "5 LEFTFIRE\n"
     ]
    }
   ],
   "source": [
    "for action_id, name in enumerate(env.unwrapped.get_action_meanings()):\n",
    "    print(action_id, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOOP, RIGHT, LEFT= 0, 2, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAADSCAIAAABCR1ywAAACh0lEQVR4nO3bPUoDURhAUSMD2lu4CDdgaZeVWNq6GVeSzjIbcBEp0sfOJsg0EgPODLmeUwXyMw8uHw/mTVZXP3h5ffjpLS7Ias6Qz0+nr/X2/jHDSuZx+Nyc/MztzXrSNVxP+ussTuA4geOGpS483mt/szdfuvFe+5u9+a+Y4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOW+xe9H+4/zw25/3nMRMcJ3CcwHGzPpPF/ExwnMBxw3a3X3oNTMgExwkcJ3CcwHECxwkcJ3CcwHECxw2P93dLr4EJmeA4geMEjhM4TuA4geMEjhM4TuA4geOOgbe7vafvkkxwnMBxAscd/x/s0LDKBMcJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHDcsvQBOO3xuvl/f3qzP+q4JjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4geMEjhM4TuA458EX4Nwz4DETHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHDdsd/ul18CETHCcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECx30Bu1ci6JJ73gAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=160x210 at 0x7FB8F4515F60>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.fromarray(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, False, {'ale.lives': 0}, (84, 84))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "observation, reward, done, info = env.step(0)\n",
    "frame = preprocessor.rgb2gray(observation)\n",
    "plt.imshow(frame, cmap = \"gray\")\n",
    "reward, done, info, frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9217828124999999"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 92\n",
    "# t = D.D_win[i][0][0][0].cpu().numpy()\n",
    "# plt.imshow(t, cmap = \"gray\")\n",
    "# time.sleep(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, False, {'ale.lives': 0}, (84, 84), torch.Size([1, 4, 84, 84]))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "observation, reward, done, info = env.step(3)\n",
    "frame = preprocessor.rgb2gray(observation)\n",
    "frames = preprocessor.phi(observation)\n",
    "plt.imshow(frame, cmap = \"gray\")\n",
    "reward, done, info, frame.shape, frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test preprocess:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images,):\n",
    "    assert len(images) == 4\n",
    "    \n",
    "    # Create figure with 3x3 sub-plots.\n",
    "    fig, axes = plt.subplots(2, 2)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i], cmap='binary')\n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "def show_image_from_frames(frames):\n",
    "    images = np.array(frames.cpu())[0]\n",
    "    plot_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATEAAADuCAYAAABRejAmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAABHNJREFUeJzt3NFt4koYgFH7KiWwz+si6AlREXJPFJF9XnrwrQApJJDZLz7n2Rr+h9GnGWIyb9s2AVT9N3oAgK8QMSBNxIA0EQPSRAxIEzEgTcSANBED0kQMSHt75OHD4bAty/KiUbjn/f19ut1u8+g59sieH+ORPf9QxJZlma7X6+em4tOOx+PoEXbLnh/jkT3vOgmkiRiQJmJAmogBaSIGpIkYkCZiQJqIAWkiBqSJGJAmYkCaiAFpIgakiRiQJmJAmogBaSIGpIkYkCZiQJqIAWkiBqSJGJAmYkCaiAFpIgakiRiQJmJAmogBaSIGpIkYkCZiQJqIAWkiBqSJ2EDruk7ruo4eA9JEDEgTMSBNxIA0EQPSRAxIEzEgTcSANBED0kQM+DaveMH77amr8ZDT6TR6BMhzEgPSRAxIe3rELpfLdLlcnr3sl8zzPHoE4EWcxIC0XURs27bRI/CD/Uu3jz3eOnYRMeDnEjH4QfZ46/CeGPBtXvFupJMYkCZiQJqIAWlP/07sfD4/e0mAu5zEgDR/nYQvcvsYy0kMSBMxIE3EgDQRA9JEDEgTMSBNxIA0EQPSRAxIEzEgTcSANBED0kQMSBMxIE3EgDQRA9JEDEgTMSBNxIA0EQPSRAxIEzEgTcSANBED0kQMSBMxIE3EgDQRA9JEDEgTMSBNxIA0EQPSRAxIEzEgbd627eMPz/PfaZr+vG4c7vi9bduv0UPskT0/zIf3/EMRA/jXuE4CaSIGpIkYkCZiQJqIAWkiBqSJGJAmYkCaiAFpIgakiRiQJmJAmogBaSIGpIkYkCZiQJqIAWkiBqSJGJAmYkCaiAFpIgakiRiQJmJAmogBaSIGpIkYkPb2yMOHw2FbluVFo3DP+/v7dLvd5tFz7JE9P8Yje/6hiC3LMl2v189Nxacdj8fRI+yWPT/GI3vedRJIEzEgTcSANBED0kQMSBMxIE3EgDQRA9JEDEgTMSBNxIA0EQPSRAxIEzEgTcSANBED0kQMSBMxIE3EgDQRA9JEDEgTMSBNxIA0EQPSRAxIEzEgTcSANBED0kQMSBMxIE3EgDQRA9JEDEgTMSBNxIA0EQPSRAxIEzEgTcSANBED0kQMSBMxIE3EgDQRA9JEDEgTMSBNxIA0EQPSRAxIEzEg7ekRu1wu0+VyefayD5nneejnA9/HSQxI+5ER27Zt9AjsyMjbh1vHD40YsB8iBqSJ2EDruk7ruo4egzBfnYgYECdiQJqIAWlvz17wfD4/e0mAu5zEgLSnn8Rgb9w+xnISA9JEDEhznRzodDqNHgHynMSANBED0kQM+Dav+L2wiAFpIgakiRiQJmJAmogBaSIGpIkYkCZiQJqIAWl+AA58m1f80wMnMSBNxIA0EQPSRAxIEzEgTcSANBED0kQMSBMxIE3EgDQRA9JEDEgTMSBNxIC0edu2jz88z3+nafrzunG44/e2bb9GD7FH9vwwH97zD0UM4F/jOgmkiRiQJmJAmogBaSIGpIkYkCZiQJqIAWkiBqT9DyulgKG7+0IbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image_from_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0],32,8,stride=4),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32,64,4, stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64,64,3, stride=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(512, n_actions))\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimizer(Q_net, frames, lr=0.1, n_iter=1, is_train = [True, False, False]):\n",
    "    optimizer = torch.optim.RMSprop(Q_net.parameters(), lr=lr)\n",
    "    for i in range(n_iter):\n",
    "        loss = None\n",
    "        y_hat = Q_net(frames)\n",
    "        print(f\"y_hat {y_hat}.\")\n",
    "        idx = torch.LongTensor([[0]]).to(device)\n",
    "        y_0 = torch.gather(y_hat, 1, idx)\n",
    "        print(f\"y_0 {y_0}.\")\n",
    "        idx = torch.LongTensor([[1]]).to(device)\n",
    "        y_1 = torch.gather(y_hat, 1, idx)\n",
    "        print(f\"y_1 {y_1}\")\n",
    "        idx = torch.LongTensor([[2]]).to(device)\n",
    "        y_2 = torch.gather(y_hat, 1, idx)\n",
    "        print(f\"y_2 {y_2}\")\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if is_train[0]:\n",
    "            y =  torch.from_numpy(np.array([-1])).float().to(device)\n",
    "            loss1 = nn.MSELoss()(y_0, y)\n",
    "            if loss:\n",
    "                loss += loss1\n",
    "            else:\n",
    "                loss = loss1\n",
    "        \n",
    "        if is_train[1]:\n",
    "            y =  torch.from_numpy(np.array([1])).float().to(device)\n",
    "            loss2 = nn.MSELoss()(y_1, y)\n",
    "            if loss:\n",
    "                loss += loss2\n",
    "            else:\n",
    "                loss = loss2\n",
    "                \n",
    "        if is_train[2]:\n",
    "            y =  torch.from_numpy(np.array([1])).float().to(device)\n",
    "            loss3 = nn.MSELoss()(y_2, y)\n",
    "            if loss:\n",
    "                loss += loss3\n",
    "            else:\n",
    "                loss = loss3\n",
    "                \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"new y_hat {Q_net(frames)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat tensor([[ 0.0228,  0.0792, -0.0545]], grad_fn=<AddmmBackward>).\n",
      "y_0 tensor([[0.0228]], grad_fn=<GatherBackward>).\n",
      "y_1 tensor([[0.0792]], grad_fn=<GatherBackward>)\n",
      "y_2 tensor([[-0.0545]], grad_fn=<GatherBackward>)\n",
      "new y_hat tensor([[-1.1337e+08,  1.1334e+08,  1.1332e+08]], grad_fn=<AddmmBackward>)\n",
      "y_hat tensor([[-1.1337e+08,  1.1334e+08,  1.1332e+08]], grad_fn=<AddmmBackward>).\n",
      "y_0 tensor([[-1.1337e+08]], grad_fn=<GatherBackward>).\n",
      "y_1 tensor([[1.1334e+08]], grad_fn=<GatherBackward>)\n",
      "y_2 tensor([[1.1332e+08]], grad_fn=<GatherBackward>)\n",
      "new y_hat tensor([[ 15538628., -15527594., -15537142.]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "Q_net = DQN([4], 3) \n",
    "run_optimizer(Q_net, frames, lr=1, n_iter=2, is_train=[True, True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat tensor([[-0.0037, -0.0609, -0.0716]], grad_fn=<AddmmBackward>).\n",
      "y_0 tensor([[-0.0037]], grad_fn=<GatherBackward>).\n",
      "y_1 tensor([[-0.0609]], grad_fn=<GatherBackward>)\n",
      "y_2 tensor([[-0.0716]], grad_fn=<GatherBackward>)\n",
      "new y_hat tensor([[-404.1568,  396.7926,  400.1288]], grad_fn=<AddmmBackward>)\n",
      "y_hat tensor([[-404.1568,  396.7926,  400.1288]], grad_fn=<AddmmBackward>).\n",
      "y_0 tensor([[-404.1568]], grad_fn=<GatherBackward>).\n",
      "y_1 tensor([[396.7926]], grad_fn=<GatherBackward>)\n",
      "y_2 tensor([[400.1288]], grad_fn=<GatherBackward>)\n",
      "new y_hat tensor([[ 102.8662, -110.1030, -109.5303]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "Q_net = DQN([4], 3) \n",
    "run_optimizer(Q_net, frames, lr=0.01, n_iter=2, is_train=[True, True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat tensor([[-0.0398, -0.1603,  0.2148]], grad_fn=<AddmmBackward>).\n",
      "y_0 tensor([[-0.0398]], grad_fn=<GatherBackward>).\n",
      "y_1 tensor([[-0.1603]], grad_fn=<GatherBackward>)\n",
      "y_2 tensor([[0.2148]], grad_fn=<GatherBackward>)\n",
      "new y_hat tensor([[-1.6907,  1.6267,  1.4938]], grad_fn=<AddmmBackward>)\n",
      "y_hat tensor([[-1.6907,  1.6267,  1.4938]], grad_fn=<AddmmBackward>).\n",
      "y_0 tensor([[-1.6907]], grad_fn=<GatherBackward>).\n",
      "y_1 tensor([[1.6267]], grad_fn=<GatherBackward>)\n",
      "y_2 tensor([[1.4938]], grad_fn=<GatherBackward>)\n",
      "new y_hat tensor([[-0.3705,  0.7817,  0.5067]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "Q_net = DQN([4], 3) \n",
    "run_optimizer(Q_net, frames, lr=0.0001, n_iter=2, is_train=[True, True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat tensor([[ 0.1684,  0.0417, -0.0422]], grad_fn=<AddmmBackward>).\n",
      "y_0 tensor([[0.1684]], grad_fn=<GatherBackward>).\n",
      "y_1 tensor([[0.0417]], grad_fn=<GatherBackward>)\n",
      "y_2 tensor([[-0.0422]], grad_fn=<GatherBackward>)\n",
      "new y_hat tensor([[-0.3375,  0.5080,  0.5443]], grad_fn=<AddmmBackward>)\n",
      "y_hat tensor([[-0.3375,  0.5080,  0.5443]], grad_fn=<AddmmBackward>).\n",
      "y_0 tensor([[-0.3375]], grad_fn=<GatherBackward>).\n",
      "y_1 tensor([[0.5080]], grad_fn=<GatherBackward>)\n",
      "y_2 tensor([[0.5443]], grad_fn=<GatherBackward>)\n",
      "new y_hat tensor([[-0.6437,  0.6848,  0.7033]], grad_fn=<AddmmBackward>)\n",
      "y_hat tensor([[-0.6437,  0.6848,  0.7033]], grad_fn=<AddmmBackward>).\n",
      "y_0 tensor([[-0.6437]], grad_fn=<GatherBackward>).\n",
      "y_1 tensor([[0.6848]], grad_fn=<GatherBackward>)\n",
      "y_2 tensor([[0.7033]], grad_fn=<GatherBackward>)\n",
      "new y_hat tensor([[-0.7459,  0.8427,  0.8287]], grad_fn=<AddmmBackward>)\n",
      "y_hat tensor([[-0.7459,  0.8427,  0.8287]], grad_fn=<AddmmBackward>).\n",
      "y_0 tensor([[-0.7459]], grad_fn=<GatherBackward>).\n",
      "y_1 tensor([[0.8427]], grad_fn=<GatherBackward>)\n",
      "y_2 tensor([[0.8287]], grad_fn=<GatherBackward>)\n",
      "new y_hat tensor([[-0.8704,  0.8784,  0.8904]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "Q_net = DQN([4], 3) \n",
    "run_optimizer(Q_net, frames, lr=0.00003, n_iter=4, is_train=[True, True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good learning rate is about 0.00003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, batch_size, win_buffer_start_size=1000, loss_buffer_start_size=3001, buffer_capacity=40000):\n",
    "        self.batch_size = batch_size\n",
    "        self.D_win = []\n",
    "        self.D_lose = []\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.loss_buffer_start_size = loss_buffer_start_size\n",
    "        self.win_buffer_start_size = win_buffer_start_size\n",
    "        self.game = []\n",
    "        \n",
    "    def reset_game(self,):\n",
    "        self.game = []\n",
    "        \n",
    "    def add_sample(self, sample):\n",
    "        if sample:\n",
    "            self.game.append(sample)\n",
    "            reward = sample[2]\n",
    "            \n",
    "            if reward == -1:\n",
    "                for step in self.game[30:]:\n",
    "                    self.D_lose.append(step)\n",
    "                self.reset_game()\n",
    "                if len(self.D_lose) > self.buffer_capacity:\n",
    "                    self.D_lose = self.D_lose[-(self.buffer_capacity // 2):]\n",
    "            if reward == 1:\n",
    "                for step in self.game[30:]:\n",
    "                    self.D_win.append(step)\n",
    "                self.reset_game()\n",
    "                if len(self.D_win) > self.buffer_capacity:\n",
    "                    self.D_win = self.D_win[-(self.buffer_capacity // 2):]\n",
    "            \n",
    "    def get_sample(self):\n",
    "        batch = None\n",
    "        is_mixed = True\n",
    "        if len(self.D_win) > self.win_buffer_start_size:\n",
    "            batch_win = random.sample(self.D_win, self.batch_size // 2)\n",
    "            batch_lose = random.sample(self.D_lose, self.batch_size // 2)\n",
    "            batch = batch_win + batch_lose\n",
    "        elif len(self.D_lose) > self.loss_buffer_start_size:\n",
    "            is_mixed = False\n",
    "            batch = random.sample(self.D_lose, self.batch_size)\n",
    "        return batch, is_mixed\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.D_win), len(self.D_lose)\n",
    "    \n",
    "    def __getitem__(self, idx, is_win=False):\n",
    "        if is_win:\n",
    "            D = self.D_win\n",
    "        else:\n",
    "            D = self.D_lose\n",
    "        if idx < 0 or idx > len(D) - 1:\n",
    "            return None\n",
    "        return D[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 1: deep Q-learning with experience replay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy():\n",
    "    def __init__(self, Q, n_action, device, win_gamma=1, loss_gamma=0.9, epsilon=1, alpha=0.8, learning_rate=0.003):\n",
    "        self.win_gamma = win_gamma\n",
    "        self.loss_gamma = loss_gamma\n",
    "        self.episode = 1\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.EPS_DECAY_RATE = 0.9999\n",
    "        self.n_action = n_action\n",
    "        self.learning_rate = learning_rate\n",
    "        self.action_mapper = [NOOP, RIGHT, LEFT]\n",
    "        self.final_exploration = 0.1\n",
    "        self.device = device\n",
    "        # Step 1: Initilize Q(State, Action)\n",
    "        self.net = Q.to(device)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.RMSprop(self.net.parameters(), lr=learning_rate)\n",
    "        self.model_path = './models/DQN/checkpoint.pth.tar'\n",
    "        \n",
    "    def update_epsilon(self,):\n",
    "        self.epsilon *= self.EPS_DECAY_RATE\n",
    "        \n",
    "    def save_model(self,):\n",
    "        torch.save({'state_dict': self.net.state_dict()}, self.model_path)\n",
    "        \n",
    "    def load_model(self,):\n",
    "        if os.path.isfile(self.model_path):\n",
    "            checkpoint = torch.load(self.model_path)\n",
    "            self.net.load_state_dict(checkpoint['state_dict'])\n",
    "            self.net = self.net.to(self.device)\n",
    "        \n",
    "    def to_tensor(self, frames):\n",
    "        frame = torch.from_numpy(np.stack(frames)).float().to(self.device)\n",
    "        return frame\n",
    "    \n",
    "    def update_state_dict(self, Q):\n",
    "        self.net.load_state_dict(Q.net.state_dict())\n",
    "        \n",
    "    def get_y_hat(self, batch):\n",
    "        batch_state = [sample[0] for sample in batch]\n",
    "        y_Q = self.net(torch.cat(batch_state))\n",
    "\n",
    "        batch_a = [[sample[1]] for sample in batch]\n",
    "        idx = torch.LongTensor(batch_a).to(device)\n",
    "        y_Q = torch.gather(y_Q, 1, idx)\n",
    "#         state_action_values = self.moving_nn(states_t).gather(1, actions_t[:,None]).squeeze(-1)\n",
    "        return y_Q\n",
    "    \n",
    "    def update_Q(self, y, batch):\n",
    "        y_hat_Q = self.get_y_hat(batch)\n",
    "        loss = self.loss_fn(y_hat_Q, y)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def get_target(self, batch, is_mixed=False, verbose=False):\n",
    "        r_t = torch.from_numpy(np.array([sample[2] for sample in batch])).float().to(self.device)\n",
    "        if verbose:\n",
    "            print('r_t', r_t)\n",
    "        s_t_plus_1 = [sample[3] for sample in batch]\n",
    "        \n",
    "        q_t_plus_1 = self.net(torch.cat(s_t_plus_1))\n",
    "        if verbose:\n",
    "            print('q_t_plus_1 for all action: ', q_t_plus_1)\n",
    "        q_t_plus_1 = q_t_plus_1.max(dim=1)[0]\n",
    "        if verbose:\n",
    "            print('q_t_plus_1', q_t_plus_1)\n",
    "        if is_mixed:\n",
    "            q_t_plus_1[0:len(q_t_plus_1)//2] = q_t_plus_1[0:len(q_t_plus_1)//2] * self.win_gamma\n",
    "            q_t_plus_1[len(q_t_plus_1)//2:] = q_t_plus_1[len(q_t_plus_1)//2:] * self.loss_gamma\n",
    "        else:\n",
    "            q_t_plus_1 = q_t_plus_1 * self.loss_gamma\n",
    "        if verbose:\n",
    "            print('q_t_plus_1 with gamma', q_t_plus_1)\n",
    "        idx = (r_t != -1) & (r_t != 1)\n",
    "        r_t[idx] += q_t_plus_1[idx]\n",
    "        # do net train target value\n",
    "        return r_t\n",
    "    \n",
    "    def get_next_action(self, state, is_e_greedy=False):\n",
    "        if is_e_greedy and np.random.random() < max(self.epsilon, self.final_exploration):\n",
    "            next_action = np.random.randint(self.n_action)\n",
    "            return next_action, self.action_mapper[next_action]\n",
    "        \n",
    "        output = self.net(state)\n",
    "        _, next_action = torch.max(output, dim=1)\n",
    "        next_action = next_action.item()\n",
    "        return next_action, self.action_mapper[next_action]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(D.D_lose) > D.buffer_capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(len(D.D_win))\n",
    "# batch, _ = D.get_sample(is_win=True)\n",
    "# target = Q_hat.get_target(batch, verbose=True)\n",
    "# y_hat = Q.get_y_hat(batch)\n",
    "# target, y_hat.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch, _ = D.get_sample(is_win=False)\n",
    "# target = Q_hat.get_target(batch, verbose=True)\n",
    "# y_hat = Q.get_y_hat(batch)\n",
    "# target, y_hat.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor('pong', 210, 160, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = DataLoader(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=1\n",
    "Q = Policy(DQN([4,84,84], 3), 3, device, epsilon=epsilon)\n",
    "Q.load_model()\n",
    "Q_hat = Policy(DQN([4,84,84], 3), 3, device, epsilon=epsilon)\n",
    "Q_hat.update_state_dict(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env, Q, preprocessor):\n",
    "    observation = env.reset()\n",
    "\n",
    "    for i in range(5):\n",
    "        observation, reward, done, info = env.step(0)\n",
    "        phi_t = preprocessor.phi(observation)\n",
    "    for t in range(10000):\n",
    "        time.sleep(0.1)\n",
    "        env.render()\n",
    "        index_a_t, a_t = Q.get_next_action(phi_t, is_e_greedy=False)\n",
    "        time.sleep(0.1)\n",
    "        x_t_plus_1, r_t, done, info = env.step(a_t)\n",
    "        phi_t = preprocessor.phi(x_t_plus_1)\n",
    "        if r_t == -1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_win = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "C = 3\n",
    "MAX_N_GAMES = 1\n",
    "scores = [0,0]\n",
    "counter = 0\n",
    "for episode in range(MAX_N_GAMES):\n",
    "    observation = env.reset()\n",
    "    \n",
    "    for i in range(5):\n",
    "        observation, reward, done, info = env.step(0)\n",
    "        phi_t = preprocessor.phi(observation)\n",
    "    \n",
    "    for t in range(10000):\n",
    "        # get next action:\n",
    "        index_a_t, a_t = Q.get_next_action(phi_t, is_e_greedy=True)\n",
    "        if n_win > 100:\n",
    "            Q.update_epsilon()\n",
    "        # get next state and reward:\n",
    "        x_t_plus_1, r_t, done, info = env.step(a_t)\n",
    "        if r_t == -1:\n",
    "            scores[0]+=1\n",
    "        if r_t == 1:\n",
    "            scores[1]+=1\n",
    "            n_win+=1\n",
    "        phi_t_plus_1 = preprocessor.phi(x_t_plus_1)\n",
    "        # enrich dataset:\n",
    "        D.add_sample((phi_t, index_a_t, r_t, phi_t_plus_1))\n",
    "        \n",
    "        # update value:np.random.random()\n",
    "        batch, is_mixed = D.get_sample()\n",
    "        if batch:\n",
    "            y = Q_hat.get_target(batch, is_mixed=is_mixed)\n",
    "            y = y.detach()\n",
    "            Q.update_Q(y, batch)\n",
    "        # reset state for new loop\n",
    "        phi_t = phi_t_plus_1\n",
    "        counter += 1\n",
    "        if (episode % C) == 0:\n",
    "            Q_hat.update_state_dict(Q)\n",
    "        if done:\n",
    "            print(f\"episode: {episode}, score: agent vs AI: {scores[0]} - {scores[1]}\")\n",
    "            scores = [0,0]\n",
    "            break\n",
    "    if (episode + 1) % 500 == 0:\n",
    "        print(\"Save model!\")\n",
    "        Q.save_model()\n",
    "#         play(env, Q, preprocessor)\n",
    "Q_hat.update_state_dict(Q)\n",
    "Q.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch, is_mixed = D.get_sample()\n",
    "print(is_mixed)\n",
    "target = Q_hat.get_target(batch, is_mixed=is_mixed,verbose=True)\n",
    "y_hat = Q.get_y_hat(batch)\n",
    "target, y_hat.view(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
